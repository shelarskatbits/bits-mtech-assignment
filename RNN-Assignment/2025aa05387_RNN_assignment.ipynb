{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7dc017f1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n================================================================================\\nDEEP NEURAL NETWORKS - ASSIGNMENT 3: RNN vs TRANSFORMER FOR TIME SERIES\\nRecurrent Neural Networks vs Transformers for Time Series Prediction\\n================================================================================\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "DEEP NEURAL NETWORKS - ASSIGNMENT 3: RNN vs TRANSFORMER FOR TIME SERIES\n",
        "Recurrent Neural Networks vs Transformers for Time Series Prediction\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "23b67847",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n================================================================================\\nSTUDENT INFORMATION (REQUIRED - DO NOT DELETE)\\n================================================================================\\n\\nBITS ID: [Enter your BITS ID here - e.g., 2025AA05036]\\nName: [Enter your full name here - e.g., JOHN DOE]\\nEmail: [Enter your email]\\nDate: [Submission date]\\n\\n================================================================================\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "STUDENT INFORMATION (REQUIRED - DO NOT DELETE)\n",
        "================================================================================\n",
        "\n",
        "BITS ID: [Enter your BITS ID here - e.g., 2025AA05036]\n",
        "Name: [Enter your full name here - e.g., JOHN DOE]\n",
        "Email: [Enter your email]\n",
        "Date: [Submission date]\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b5a0dc29",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n================================================================================\\nASSIGNMENT OVERVIEW\\n================================================================================\\n\\nThis assignment requires you to implement and compare two approaches for \\ntime series forecasting:\\n1. LSTM or GRU using Keras/PyTorch\\n2. Transformer encoder using Keras/PyTorch layers\\n\\nLearning Objectives:\\n- Build recurrent neural networks for sequential data\\n- Use transformer architecture for time series\\n- Implement or integrate positional encoding\\n- Compare RNN vs Transformer architectures\\n- Understand time series preprocessing and evaluation\\n\\nIMPORTANT: \\n- Positional encoding MUST be added to transformer\\n- Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\\n- DO NOT use pre-trained transformers (HuggingFace, TimeGPT, etc.)\\n- Use temporal train/test split (NO shuffling)\\n\\n================================================================================\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "ASSIGNMENT OVERVIEW\n",
        "================================================================================\n",
        "\n",
        "This assignment requires you to implement and compare two approaches for \n",
        "time series forecasting:\n",
        "1. LSTM or GRU using Keras/PyTorch\n",
        "2. Transformer encoder using Keras/PyTorch layers\n",
        "\n",
        "Learning Objectives:\n",
        "- Build recurrent neural networks for sequential data\n",
        "- Use transformer architecture for time series\n",
        "- Implement or integrate positional encoding\n",
        "- Compare RNN vs Transformer architectures\n",
        "- Understand time series preprocessing and evaluation\n",
        "\n",
        "IMPORTANT: \n",
        "- Positional encoding MUST be added to transformer\n",
        "- Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\n",
        "- DO NOT use pre-trained transformers (HuggingFace, TimeGPT, etc.)\n",
        "- Use temporal train/test split (NO shuffling)\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c4b05f5b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n================================================================================\\n⚠️ IMPORTANT SUBMISSION REQUIREMENTS - STRICTLY ENFORCED ⚠️\\n================================================================================\\n\\n1. FILENAME FORMAT: <BITS_ID>_rnn_assignment.ipynb\\n   Example: 2025AA05036_rnn_assignment.ipynb\\n   ❌ Wrong filename = Automatic 0 marks\\n\\n2. STUDENT INFORMATION MUST MATCH:\\n   ✓ BITS ID in filename = BITS ID in notebook (above)\\n   ✓ Name in folder = Name in notebook (above)\\n   ❌ Mismatch = 0 marks\\n\\n3. EXECUTE ALL CELLS BEFORE SUBMISSION:\\n   - Run: Kernel → Restart & Run All\\n   - Verify all outputs are visible\\n   ❌ No outputs = 0 marks\\n\\n4. FILE INTEGRITY:\\n   - Ensure notebook opens without errors\\n   - Check for corrupted cells\\n   ❌ Corrupted file = 0 marks\\n\\n5. IMPLEMENTATION REQUIREMENTS:\\n   - MUST add positional encoding to transformer (custom or built-in)\\n   - CAN use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\\n   - DO NOT use pre-trained transformers (HuggingFace, TimeGPT, etc.)\\n   - DO NOT shuffle time series data (temporal order required)\\n   ❌ Missing positional encoding = 0 marks for transformer section\\n\\n6. DATASET REQUIREMENTS:\\n   - Minimum 1000 time steps\\n   - Train/test split: 90/10 OR 85/15 (temporal split only)\\n   - Sequence length: 10-50 time steps\\n   - Prediction horizon: 1-10 time steps\\n\\n7. USE KERAS OR PYTORCH:\\n   - Use framework's LSTM/GRU layers\\n   - Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\\n   - Add positional encoding (custom implementation or built-in)\\n   - Use standard training methods\\n\\n8. FILE SUBMISSION:\\n   - Submit ONLY the .ipynb file\\n   - NO zip files, NO separate data files, NO separate image files\\n   - All code and outputs must be in the notebook\\n   - Only one submission attempt allowed\\n\\n================================================================================\\n\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "⚠️ IMPORTANT SUBMISSION REQUIREMENTS - STRICTLY ENFORCED ⚠️\n",
        "================================================================================\n",
        "\n",
        "1. FILENAME FORMAT: <BITS_ID>_rnn_assignment.ipynb\n",
        "   Example: 2025AA05036_rnn_assignment.ipynb\n",
        "   ❌ Wrong filename = Automatic 0 marks\n",
        "\n",
        "2. STUDENT INFORMATION MUST MATCH:\n",
        "   ✓ BITS ID in filename = BITS ID in notebook (above)\n",
        "   ✓ Name in folder = Name in notebook (above)\n",
        "   ❌ Mismatch = 0 marks\n",
        "\n",
        "3. EXECUTE ALL CELLS BEFORE SUBMISSION:\n",
        "   - Run: Kernel → Restart & Run All\n",
        "   - Verify all outputs are visible\n",
        "   ❌ No outputs = 0 marks\n",
        "\n",
        "4. FILE INTEGRITY:\n",
        "   - Ensure notebook opens without errors\n",
        "   - Check for corrupted cells\n",
        "   ❌ Corrupted file = 0 marks\n",
        "\n",
        "5. IMPLEMENTATION REQUIREMENTS:\n",
        "   - MUST add positional encoding to transformer (custom or built-in)\n",
        "   - CAN use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\n",
        "   - DO NOT use pre-trained transformers (HuggingFace, TimeGPT, etc.)\n",
        "   - DO NOT shuffle time series data (temporal order required)\n",
        "   ❌ Missing positional encoding = 0 marks for transformer section\n",
        "\n",
        "6. DATASET REQUIREMENTS:\n",
        "   - Minimum 1000 time steps\n",
        "   - Train/test split: 90/10 OR 85/15 (temporal split only)\n",
        "   - Sequence length: 10-50 time steps\n",
        "   - Prediction horizon: 1-10 time steps\n",
        "\n",
        "7. USE KERAS OR PYTORCH:\n",
        "   - Use framework's LSTM/GRU layers\n",
        "   - Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\n",
        "   - Add positional encoding (custom implementation or built-in)\n",
        "   - Use standard training methods\n",
        "\n",
        "8. FILE SUBMISSION:\n",
        "   - Submit ONLY the .ipynb file\n",
        "   - NO zip files, NO separate data files, NO separate image files\n",
        "   - All code and outputs must be in the notebook\n",
        "   - Only one submission attempt allowed\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f7883103",
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_46504/2437490422.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# PyTorch for RNN and Transformer implementation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "# Import Required Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "\n",
        "# PyTorch for RNN and Transformer implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17b889b4",
      "metadata": {},
      "source": [
        "# Framework: PyTorch (used for both LSTM and Transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002c89de",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 1: DATASET LOADING AND EXPLORATION (Informational)\n",
        "================================================================================\n",
        "\n",
        "Instructions:\n",
        "1. Choose ONE dataset from the allowed list\n",
        "2. Load and explore the time series data\n",
        "3. Fill in ALL required metadata fields below\n",
        "4. Provide justification for your primary metric choice\n",
        "\n",
        "ALLOWED DATASETS:\n",
        "- Stock Prices (daily/hourly closing prices)\n",
        "- Weather Data (temperature, humidity, pressure)\n",
        "- Energy Consumption (electricity/power usage)\n",
        "- Sensor Data (IoT sensor readings)\n",
        "- Custom time series (with approval)\n",
        "\n",
        "REQUIRED OUTPUT:\n",
        "- Print all metadata fields\n",
        "- Time series plots\n",
        "- Stationarity analysis\n",
        "- Train/test split visualization\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c8fef3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1 Dataset Selection and Loading - Tesla Stock (Close price)\n",
        "csv_path = 'tesla-stock-prediction-data.csv'\n",
        "data = pd.read_csv(csv_path)\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data = data.sort_values('Date').reset_index(drop=True)\n",
        "raw_data = data['Close'].values.reshape(-1, 1).astype(np.float32)\n",
        "dates = data['Date'].values\n",
        "\n",
        "print(f\"Dataset loaded: {len(raw_data)} samples\")\n",
        "print(f\"Data shape: {raw_data.shape}\")\n",
        "print(f\"Date range: {data['Date'].min().date()} to {data['Date'].max().date()}\")\n",
        "print(f\"First 5 Close values: {raw_data[:5].flatten()}\")\n",
        "print(f\"Last 5 Close values: {raw_data[-5:].flatten()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b9ac13",
      "metadata": {},
      "outputs": [],
      "source": [
        "# REQUIRED: Fill in these metadata fields\n",
        "dataset_name = \"Tesla Stock Price (Daily Close)\"\n",
        "dataset_source = \"tesla-stock-prediction-data.csv (same folder as notebook)\"\n",
        "n_samples = len(raw_data)  # Total number of time steps\n",
        "n_features = 1  # Number of features (1 for univariate, >1 for multivariate)\n",
        "sequence_length = 30  # Lookback window (10-50)\n",
        "prediction_horizon = 1  # Forecast steps ahead (1-10)\n",
        "problem_type = \"time_series_forecasting\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ae533b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Primary metric selection\n",
        "primary_metric = \"MAE\"\n",
        "metric_justification = \"\"\"\n",
        "MAE is chosen as the primary metric because it provides a straightforward \n",
        "interpretation of average prediction error magnitude, which is important \n",
        "for financial forecasting where understanding typical error size matters \n",
        "more than penalizing outliers.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fef57a2",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATASET INFORMATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Dataset: {dataset_name}\")\n",
        "print(f\"Source: {dataset_source}\")\n",
        "print(f\"Total Samples: {n_samples}\")\n",
        "print(f\"Number of Features: {n_features}\")\n",
        "print(f\"Sequence Length: {sequence_length}\")\n",
        "print(f\"Prediction Horizon: {prediction_horizon}\")\n",
        "print(f\"Primary Metric: {primary_metric}\")\n",
        "print(f\"Metric Justification: {metric_justification}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfd8ddfa",
      "metadata": {},
      "source": [
        "# 1.2 Time Series Exploration\n",
        "# Plot time series data\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(dates, raw_data, linewidth=0.5)\n",
        "plt.title('Time Series Data: Stock Prices')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot first 500 points for detail\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(dates[:500], raw_data[:500], linewidth=1)\n",
        "plt.title('Time Series Data: First 500 Points (Detail View)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nData Statistics:\")\n",
        "print(f\"Mean: {np.mean(raw_data):.2f}\")\n",
        "print(f\"Std: {np.std(raw_data):.2f}\")\n",
        "print(f\"Min: {np.min(raw_data):.2f}\")\n",
        "print(f\"Max: {np.max(raw_data):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d498e253",
      "metadata": {},
      "source": [
        "1.3 Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6efdacdc",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def preprocess_timeseries(data):\n",
        "    \"\"\"\n",
        "    Preprocess time series data\n",
        "    \n",
        "    Args:\n",
        "        data: raw time series data\n",
        "    \n",
        "    Returns:\n",
        "        preprocessed data, scaler\n",
        "    \"\"\"\n",
        "    # Normalize/standardize data\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = scaler.fit_transform(data)\n",
        "    \n",
        "    # Handle missing values if any (check for NaN)\n",
        "    if np.isnan(data_scaled).any():\n",
        "        print(\"Warning: NaN values found, filling with forward fill\")\n",
        "        data_scaled = pd.DataFrame(data_scaled).fillna(method='ffill').values\n",
        "    \n",
        "    return data_scaled, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea0da181",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def create_sequences(data, seq_length, pred_horizon):\n",
        "    \"\"\"\n",
        "    Create sequences for time series prediction\n",
        "    \n",
        "    Args:\n",
        "        data: preprocessed time series data\n",
        "        seq_length: lookback window\n",
        "        pred_horizon: forecast steps ahead\n",
        "    \n",
        "    Returns:\n",
        "        X: input sequences, y: target values\n",
        "    \"\"\"\n",
        "    # Implement sliding window approach\n",
        "    # Input: [t-n, t-n+1, ..., t-1, t]\n",
        "    # Target: [t+1] or [t+1, ..., t+h]\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length - pred_horizon + 1):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length:i+seq_length+pred_horizon].flatten())\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d888128f",
      "metadata": {},
      "source": [
        "# Preprocess data\n",
        "data_scaled, scaler = preprocess_timeseries(raw_data)\n",
        "\n",
        "# Create sequences\n",
        "X, y = create_sequences(data_scaled, sequence_length, prediction_horizon)\n",
        "\n",
        "print(f\"Sequences created:\")\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "725e3489",
      "metadata": {},
      "outputs": [],
      "source": [
        "# REQUIRED: Temporal train/test split (NO SHUFFLING)\n",
        "split_idx = int(len(X) * 0.9)\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "train_test_ratio = \"90/10\"\n",
        "train_samples = len(X_train)  # Number of training sequences\n",
        "test_samples = len(X_test)  # Number of test sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa1ee1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nTrain/Test Split: {train_test_ratio}\")\n",
        "print(f\"Training Samples: {train_samples}\")\n",
        "print(f\"Test Samples: {test_samples}\")\n",
        "print(\"⚠️  IMPORTANT: Temporal split used (NO shuffling)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ef664ae",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 2: LSTM/GRU IMPLEMENTATION (5 MARKS)\n",
        "================================================================================\n",
        "\n",
        "REQUIREMENTS:\n",
        "- Build LSTM OR GRU using Keras/PyTorch layers\n",
        "- Architecture must include:\n",
        "  * At least 2 stacked recurrent layers\n",
        "  * Output layer for prediction\n",
        "- Use model.compile() and model.fit() (Keras) OR standard PyTorch training\n",
        "- Track initial_loss and final_loss\n",
        "\n",
        "GRADING:\n",
        "- LSTM/GRU architecture with stacked layers: 2 marks\n",
        "- Model properly compiled/configured: 1 mark\n",
        "- Training completed with loss tracking: 1 mark\n",
        "- All metrics calculated correctly: 1 mark\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b349fea",
      "metadata": {},
      "source": [
        "2.1 LSTM/GRU Architecture Design\n",
        "TODO: Choose LSTM or GRU\n",
        "TODO: Design architecture with stacked layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87daaa54",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"LSTM or GRU with at least 2 stacked layers (PyTorch).\"\"\"\n",
        "    def __init__(self, model_type, n_features, hidden_units, n_layers, output_size):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_units = hidden_units\n",
        "        rnn_class = nn.LSTM if model_type == 'LSTM' else nn.GRU\n",
        "        self.rnn = rnn_class(\n",
        "            input_size=n_features,\n",
        "            hidden_size=hidden_units,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_units, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = out[:, -1, :]  # last time step\n",
        "        return self.fc(out)\n",
        "\n",
        "\n",
        "def build_rnn_model(model_type, input_shape, hidden_units, n_layers, output_size):\n",
        "    \"\"\"Build LSTM or GRU model (PyTorch). input_shape = (seq_len, n_features).\"\"\"\n",
        "    _, n_features = input_shape\n",
        "    return RNNModel(model_type, n_features, hidden_units, n_layers, output_size).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b99b76d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create RNN model\n",
        "rnn_model = build_rnn_model('LSTM', (sequence_length, n_features), 64, 2, prediction_horizon)\n",
        "print(rnn_model)\n",
        "rnn_total_params = sum(p.numel() for p in rnn_model.parameters())\n",
        "print(f\"Total parameters: {rnn_total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e07e8f69",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizer and loss (PyTorch)\n",
        "rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
        "rnn_criterion = nn.MSELoss()\n",
        "print(\"RNN optimizer and criterion set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e9e46a1",
      "metadata": {},
      "source": [
        "# Create RNN model\n",
        "rnn_model = build_rnn_model('LSTM', (sequence_length, n_features), 64, 2, prediction_horizon)\n",
        "print(rnn_model)\n",
        "rnn_total_params = sum(p.numel() for p in rnn_model.parameters())\n",
        "print(f\"Total parameters: {rnn_total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98bf8c67",
      "metadata": {},
      "source": [
        "# Optimizer and loss (PyTorch)\n",
        "rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
        "rnn_criterion = nn.MSELoss()\n",
        "print(\"RNN optimizer and criterion set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44df1c4b",
      "metadata": {},
      "source": [
        "2.2 Train RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3798c53c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train RNN (PyTorch)\n",
        "rnn_start_time = time.time()\n",
        "X_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "val_size = int(0.1 * len(X_t))\n",
        "X_tr, X_val = X_t[:-val_size], X_t[-val_size:]\n",
        "y_tr, y_val = y_t[:-val_size], y_t[-val_size:]\n",
        "train_ds = TensorDataset(X_tr, y_tr)\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=False)\n",
        "history_rnn = {'loss': [], 'val_loss': []}\n",
        "for epoch in range(50):\n",
        "    rnn_model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for bx, by in train_loader:\n",
        "        bx, by = bx.to(device), by.to(device)\n",
        "        rnn_optimizer.zero_grad()\n",
        "        out = rnn_model(bx)\n",
        "        loss = rnn_criterion(out, by)\n",
        "        loss.backward()\n",
        "        rnn_optimizer.step()\n",
        "        epoch_loss += loss.item() * bx.size(0)\n",
        "    history_rnn['loss'].append(epoch_loss / len(X_tr))\n",
        "    rnn_model.eval()\n",
        "    with torch.no_grad():\n",
        "        vout = rnn_model(X_val.to(device))\n",
        "        vloss = rnn_criterion(vout, y_val.to(device)).item()\n",
        "    history_rnn['val_loss'].append(vloss)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/50 - loss: {history_rnn['loss'][-1]:.6f} - val_loss: {vloss:.6f}\")\n",
        "rnn_training_time = time.time() - rnn_start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caebf1f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RNN MODEL TRAINING\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e65f6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Track training time\n",
        "rnn_start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4d32150",
      "metadata": {},
      "source": [
        "# Train RNN (PyTorch)\n",
        "rnn_start_time = time.time()\n",
        "X_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "val_size = int(0.1 * len(X_t))\n",
        "X_tr, X_val = X_t[:-val_size], X_t[-val_size:]\n",
        "y_tr, y_val = y_t[:-val_size], y_t[-val_size:]\n",
        "train_ds = TensorDataset(X_tr, y_tr)\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=False)\n",
        "history_rnn = {'loss': [], 'val_loss': []}\n",
        "for epoch in range(50):\n",
        "    rnn_model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for bx, by in train_loader:\n",
        "        bx, by = bx.to(device), by.to(device)\n",
        "        rnn_optimizer.zero_grad()\n",
        "        out = rnn_model(bx)\n",
        "        loss = rnn_criterion(out, by)\n",
        "        loss.backward()\n",
        "        rnn_optimizer.step()\n",
        "        epoch_loss += loss.item() * bx.size(0)\n",
        "    history_rnn['loss'].append(epoch_loss / len(X_tr))\n",
        "    rnn_model.eval()\n",
        "    with torch.no_grad():\n",
        "        vout = rnn_model(X_val.to(device))\n",
        "        vloss = rnn_criterion(vout, y_val.to(device)).item()\n",
        "    history_rnn['val_loss'].append(vloss)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/50 - loss: {history_rnn['loss'][-1]:.6f} - val_loss: {vloss:.6f}\")\n",
        "rnn_training_time = time.time() - rnn_start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b093f3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# rnn_training_time already set in training loop above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e809ab3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test set (PyTorch)\n",
        "rnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    rnn_predictions_scaled = rnn_model(X_test_t).cpu().numpy()\n",
        "rnn_predictions = scaler.inverse_transform(rnn_predictions_scaled)\n",
        "y_test_original = scaler.inverse_transform(y_test)\n",
        "print(\"RNN Predictions completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ade3a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# REQUIRED: Track initial and final loss\n",
        "rnn_initial_loss = float(history_rnn['loss'][0])\n",
        "rnn_final_loss = float(history_rnn['loss'][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49554c49",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "print(f\"Training completed in {rnn_training_time:.2f} seconds\")\n",
        "print(f\"Initial Loss: {rnn_initial_loss:.4f}\")\n",
        "print(f\"Final Loss: {rnn_final_loss:.4f}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "792870c6",
      "metadata": {},
      "source": [
        "2.3 Evaluate RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee3c417f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.4 Visualize RNN Results\n",
        "# Plot training loss curve\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_rnn['loss'], label='Training Loss')\n",
        "plt.plot(history_rnn['val_loss'], label='Validation Loss')\n",
        "plt.title('RNN Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot actual vs predicted values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(y_test_original[:100], label='Actual', alpha=0.7)\n",
        "plt.plot(rnn_predictions[:100], label='Predicted', alpha=0.7)\n",
        "plt.title('RNN: Actual vs Predicted (First 100 points)')\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f459c549",
      "metadata": {},
      "source": [
        "# Make predictions on test set (PyTorch)\n",
        "rnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    rnn_predictions_scaled = rnn_model(X_test_t).cpu().numpy()\n",
        "rnn_predictions = scaler.inverse_transform(rnn_predictions_scaled)\n",
        "y_test_original = scaler.inverse_transform(y_test)\n",
        "print(\"RNN Predictions completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1678f898",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def calculate_mape(y_true, y_pred):\n",
        "    \"\"\"Calculate Mean Absolute Percentage Error\"\"\"\n",
        "    # MAPE = mean(|y_true - y_pred| / |y_true|) * 100\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43dcb302",
      "metadata": {},
      "outputs": [],
      "source": [
        "# REQUIRED: Calculate all 4 metrics\n",
        "rnn_mae = float(mean_absolute_error(y_test_original, rnn_predictions))\n",
        "rnn_rmse = float(np.sqrt(mean_squared_error(y_test_original, rnn_predictions)))\n",
        "rnn_mape = float(calculate_mape(y_test_original, rnn_predictions))\n",
        "rnn_r2 = float(r2_score(y_test_original, rnn_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7eb4ceb",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nRNN Model Performance:\")\n",
        "print(f\"MAE:   {rnn_mae:.4f}\")\n",
        "print(f\"RMSE:  {rnn_rmse:.4f}\")\n",
        "print(f\"MAPE:  {rnn_mape:.4f}%\")\n",
        "print(f\"R² Score: {rnn_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a5be5a3",
      "metadata": {},
      "source": [
        "# 2.4 Visualize RNN Results\n",
        "# Plot training loss curve\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_rnn['loss'], label='Training Loss')\n",
        "plt.plot(history_rnn['val_loss'], label='Validation Loss')\n",
        "plt.title('RNN Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot actual vs predicted values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(y_test_original[:100], label='Actual', alpha=0.7)\n",
        "plt.plot(rnn_predictions[:100], label='Predicted', alpha=0.7)\n",
        "plt.title('RNN: Actual vs Predicted (First 100 points)')\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411d84b3",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 3: TRANSFORMER IMPLEMENTATION (5 MARKS)\n",
        "================================================================================\n",
        "\n",
        "REQUIREMENTS:\n",
        "- Build Transformer encoder using Keras/PyTorch layers\n",
        "- MUST add positional encoding to input:\n",
        "  * Custom sinusoidal implementation OR\n",
        "  * Use built-in positional encoding (if framework provides)\n",
        "- Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\n",
        "- Use standard training methods\n",
        "- Track initial_loss and final_loss\n",
        "\n",
        "PROHIBITED:\n",
        "- Using pre-trained transformers (HuggingFace, TimeGPT, etc.)\n",
        "- Skipping positional encoding entirely\n",
        "\n",
        "GRADING:\n",
        "- Positional encoding added: 1 mark\n",
        "- Transformer architecture properly configured: 2 marks\n",
        "- Training completed with loss tracking: 1 mark\n",
        "- All metrics calculated correctly: 1 mark\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "044873c0",
      "metadata": {},
      "source": [
        "3.1 Positional Encoding Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "335f5f6b",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def positional_encoding(seq_length, d_model):\n",
        "    \"\"\"Sinusoidal PE: PE(pos,2i)=sin(pos/10000^(2i/d)); PE(pos,2i+1)=cos(...).\"\"\"\n",
        "    pe = np.zeros((seq_length, d_model))\n",
        "    position = np.arange(seq_length).reshape(-1, 1)\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)\n",
        "    return pe\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"PyTorch positional encoding layer.\"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15c1783c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizer and criterion are set in the 3.2 Transformer Encoder Architecture cell below (after model is created)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83741f21",
      "metadata": {},
      "source": [
        "3.2 Transformer Encoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f1c4463",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.2 Transformer Encoder Architecture (PyTorch) - nn.TransformerEncoder + PositionalEncoding\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, n_features, d_model, n_heads, n_layers, d_ff, output_size):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(n_features, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)  # Positional encoding (mandatory)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=n_heads, dim_feedforward=d_ff,\n",
        "            batch_first=True, activation='relu'\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "        self.fc = nn.Linear(d_model, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1)\n",
        "        return self.fc(x)\n",
        "\n",
        "transformer_model = TransformerModel(\n",
        "    n_features, d_model=64, n_heads=4, n_layers=2, d_ff=256, output_size=prediction_horizon\n",
        ").to(device)\n",
        "print(transformer_model)\n",
        "transformer_total_params = sum(p.numel() for p in transformer_model.parameters())\n",
        "print(f\"Total parameters: {transformer_total_params}\")\n",
        "\n",
        "transformer_optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)\n",
        "transformer_criterion = nn.MSELoss()\n",
        "print(\"Transformer optimizer and criterion set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33435a9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option B: Using Keras\n",
        "\"\"\"\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_transformer_model(seq_length, n_features, d_model, n_heads, n_layers, d_ff, output_size):\n",
        "    inputs = layers.Input(shape=(seq_length, n_features))\n",
        "    \n",
        "    # Project to d_model\n",
        "    x = layers.Dense(d_model)(inputs)\n",
        "    \n",
        "    # Add positional encoding\n",
        "    x = x + positional_encoding(seq_length, d_model)\n",
        "    \n",
        "    # Stack transformer encoder layers\n",
        "    for _ in range(n_layers):\n",
        "        # Multi-head attention\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=n_heads, \n",
        "            key_dim=d_model // n_heads\n",
        "        )(x, x)\n",
        "        x = layers.LayerNormalization()(x + attn_output)\n",
        "        \n",
        "        # Feed-forward\n",
        "        ffn_output = layers.Dense(d_ff, activation='relu')(x)\n",
        "        ffn_output = layers.Dense(d_model)(ffn_output)\n",
        "        x = layers.LayerNormalization()(x + ffn_output)\n",
        "    \n",
        "    # Output\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    outputs = layers.Dense(output_size)(x)\n",
        "    \n",
        "    return keras.Model(inputs=inputs, outputs=outputs)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c00c3840",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Transformer (PyTorch)\n",
        "transformer_start_time = time.time()\n",
        "X_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "val_size = int(0.1 * len(X_t))\n",
        "X_tr, X_val = X_t[:-val_size], X_t[-val_size:]\n",
        "y_tr, y_val = y_t[:-val_size], y_t[-val_size:]\n",
        "train_ds_t = TensorDataset(X_tr, y_tr)\n",
        "train_loader_t = DataLoader(train_ds_t, batch_size=32, shuffle=False)\n",
        "history_transformer = {'loss': [], 'val_loss': []}\n",
        "for epoch in range(50):\n",
        "    transformer_model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for bx, by in train_loader_t:\n",
        "        bx, by = bx.to(device), by.to(device)\n",
        "        transformer_optimizer.zero_grad()\n",
        "        out = transformer_model(bx)\n",
        "        loss = transformer_criterion(out, by)\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "        epoch_loss += loss.item() * bx.size(0)\n",
        "    history_transformer['loss'].append(epoch_loss / len(X_tr))\n",
        "    transformer_model.eval()\n",
        "    with torch.no_grad():\n",
        "        vout = transformer_model(X_val.to(device))\n",
        "        vloss = transformer_criterion(vout, y_val.to(device)).item()\n",
        "    history_transformer['val_loss'].append(vloss)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/50 - loss: {history_transformer['loss'][-1]:.6f} - val_loss: {vloss:.6f}\")\n",
        "transformer_training_time = time.time() - transformer_start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ba57c3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer model and optimizer already defined in section 3.2 above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06f72750",
      "metadata": {},
      "source": [
        "3.3 Build Your Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db4f26b",
      "metadata": {},
      "source": [
        "TODO: Create Transformer model using PyTorch or Keras\n",
        "Example for PyTorch:\n",
        "transformer_model = TransformerModel(n_features, d_model=64, n_heads=4, n_layers=2, d_ff=256, output_size=prediction_horizon)\n",
        "Example for Keras:\n",
        "transformer_model = build_transformer_model(sequence_length, n_features, d_model=64, n_heads=4, n_layers=2, d_ff=256, output_size=prediction_horizon)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f1bc8c9",
      "metadata": {},
      "source": [
        "# Optimizer and loss (PyTorch)\n",
        "transformer_optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)\n",
        "transformer_criterion = nn.MSELoss()\n",
        "print(\"Transformer optimizer and criterion set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c311c74",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test set (PyTorch)\n",
        "transformer_model.eval()\n",
        "with torch.no_grad():\n",
        "    X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    transformer_predictions_scaled = transformer_model(X_test_t).cpu().numpy()\n",
        "transformer_predictions = scaler.inverse_transform(transformer_predictions_scaled)\n",
        "print(\"Transformer Predictions completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c00d6c9c",
      "metadata": {},
      "source": [
        "3.4 Train Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2325454b",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRANSFORMER MODEL TRAINING\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aee465f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.6 Visualize Transformer Results\n",
        "# Plot training loss curve\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_transformer['loss'], label='Training Loss')\n",
        "plt.plot(history_transformer['val_loss'], label='Validation Loss')\n",
        "plt.title('Transformer Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot actual vs predicted values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(y_test_original[:100], label='Actual', alpha=0.7)\n",
        "plt.plot(transformer_predictions[:100], label='Predicted', alpha=0.7)\n",
        "plt.title('Transformer: Actual vs Predicted (First 100 points)')\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26ed1263",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Track training time\n",
        "transformer_start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "846c1aab",
      "metadata": {},
      "source": [
        "# Train Transformer (PyTorch)\n",
        "transformer_start_time = time.time()\n",
        "X_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "val_size = int(0.1 * len(X_t))\n",
        "X_tr, X_val = X_t[:-val_size], X_t[-val_size:]\n",
        "y_tr, y_val = y_t[:-val_size], y_t[-val_size:]\n",
        "train_ds_t = TensorDataset(X_tr, y_tr)\n",
        "train_loader_t = DataLoader(train_ds_t, batch_size=32, shuffle=False)\n",
        "history_transformer = {'loss': [], 'val_loss': []}\n",
        "for epoch in range(50):\n",
        "    transformer_model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for bx, by in train_loader_t:\n",
        "        bx, by = bx.to(device), by.to(device)\n",
        "        transformer_optimizer.zero_grad()\n",
        "        out = transformer_model(bx)\n",
        "        loss = transformer_criterion(out, by)\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "        epoch_loss += loss.item() * bx.size(0)\n",
        "    history_transformer['loss'].append(epoch_loss / len(X_tr))\n",
        "    transformer_model.eval()\n",
        "    with torch.no_grad():\n",
        "        vout = transformer_model(X_val.to(device))\n",
        "        vloss = transformer_criterion(vout, y_val.to(device)).item()\n",
        "    history_transformer['val_loss'].append(vloss)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/50 - loss: {history_transformer['loss'][-1]:.6f} - val_loss: {vloss:.6f}\")\n",
        "transformer_training_time = time.time() - transformer_start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9171c703",
      "metadata": {},
      "outputs": [],
      "source": [
        "# transformer_training_time already set in training loop above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f9e89eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# REQUIRED: Track initial and final loss\n",
        "transformer_initial_loss = float(history_transformer['loss'][0])\n",
        "transformer_final_loss = float(history_transformer['loss'][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddfc7240",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Training completed in {transformer_training_time:.2f} seconds\")\n",
        "print(f\"Initial Loss: {transformer_initial_loss:.4f}\")\n",
        "print(f\"Final Loss: {transformer_final_loss:.4f}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3707b1e",
      "metadata": {},
      "source": [
        "3.5 Evaluate Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a6e3fa1",
      "metadata": {},
      "source": [
        "# Make predictions on test set (PyTorch)\n",
        "transformer_model.eval()\n",
        "with torch.no_grad():\n",
        "    X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    transformer_predictions_scaled = transformer_model(X_test_t).cpu().numpy()\n",
        "transformer_predictions = scaler.inverse_transform(transformer_predictions_scaled)\n",
        "print(\"Transformer Predictions completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e024b42c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# REQUIRED: Calculate all 4 metrics\n",
        "transformer_mae = float(mean_absolute_error(y_test_original, transformer_predictions))\n",
        "transformer_rmse = float(np.sqrt(mean_squared_error(y_test_original, transformer_predictions)))\n",
        "transformer_mape = float(calculate_mape(y_test_original, transformer_predictions))\n",
        "transformer_r2 = float(r2_score(y_test_original, transformer_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0126e84",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTransformer Model Performance:\")\n",
        "print(f\"MAE:   {transformer_mae:.4f}\")\n",
        "print(f\"RMSE:  {transformer_rmse:.4f}\")\n",
        "print(f\"MAPE:  {transformer_mape:.4f}%\")\n",
        "print(f\"R² Score: {transformer_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d3a7f0",
      "metadata": {},
      "source": [
        "# 3.6 Visualize Transformer Results\n",
        "# Plot training loss curve\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_transformer['loss'], label='Training Loss')\n",
        "plt.plot(history_transformer['val_loss'], label='Validation Loss')\n",
        "plt.title('Transformer Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot actual vs predicted values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(y_test_original[:100], label='Actual', alpha=0.7)\n",
        "plt.plot(transformer_predictions[:100], label='Predicted', alpha=0.7)\n",
        "plt.title('Transformer: Actual vs Predicted (First 100 points)')\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98256c4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 4: MODEL COMPARISON AND VISUALIZATION (Informational)\n",
        "================================================================================\n",
        "\n",
        "Compare both models on:\n",
        "- Performance metrics\n",
        "- Training time\n",
        "- Model complexity\n",
        "- Convergence behavior\n",
        "- Ability to capture long-term dependencies\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14e98c47",
      "metadata": {},
      "source": [
        "4.1 Metrics Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af1fbf3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc0e5a47",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate total parameters\n",
        "rnn_total_params = sum(p.numel() for p in rnn_model.parameters())\n",
        "transformer_total_params = sum(p.numel() for p in transformer_model.parameters())\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': ['MAE', 'RMSE', 'MAPE (%)', 'R² Score', 'Training Time (s)', 'Parameters'],\n",
        "    'RNN (LSTM/GRU)': [\n",
        "        rnn_mae,\n",
        "        rnn_rmse,\n",
        "        rnn_mape,\n",
        "        rnn_r2,\n",
        "        rnn_training_time,\n",
        "        rnn_total_params\n",
        "    ],\n",
        "    'Transformer': [\n",
        "        transformer_mae,\n",
        "        transformer_rmse,\n",
        "        transformer_mape,\n",
        "        transformer_r2,\n",
        "        transformer_training_time,\n",
        "        transformer_total_params\n",
        "    ]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a860057",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70ac551d",
      "metadata": {},
      "source": [
        "# 4.2 Visual Comparison\n",
        "# Create bar plot comparing metrics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# MAE and RMSE comparison\n",
        "metrics_to_plot = ['MAE', 'RMSE']\n",
        "rnn_vals = [rnn_mae, rnn_rmse]\n",
        "transformer_vals = [transformer_mae, transformer_rmse]\n",
        "\n",
        "x = np.arange(len(metrics_to_plot))\n",
        "width = 0.35\n",
        "axes[0, 0].bar(x - width/2, rnn_vals, width, label='RNN (LSTM)', alpha=0.8)\n",
        "axes[0, 0].bar(x + width/2, transformer_vals, width, label='Transformer', alpha=0.8)\n",
        "axes[0, 0].set_xlabel('Metrics')\n",
        "axes[0, 0].set_ylabel('Value')\n",
        "axes[0, 0].set_title('MAE and RMSE Comparison')\n",
        "axes[0, 0].set_xticks(x)\n",
        "axes[0, 0].set_xticklabels(metrics_to_plot)\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# R² Score comparison\n",
        "axes[0, 1].bar(['RNN (LSTM)', 'Transformer'], [rnn_r2, transformer_r2], alpha=0.8)\n",
        "axes[0, 1].set_ylabel('R² Score')\n",
        "axes[0, 1].set_title('R² Score Comparison')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Training curves comparison\n",
        "axes[1, 0].plot(history_rnn['loss'], label='RNN Train', alpha=0.7)\n",
        "axes[1, 0].plot(history_transformer['loss'], label='Transformer Train', alpha=0.7)\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Loss')\n",
        "axes[1, 0].set_title('Training Loss Comparison')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Predictions comparison (both models vs actual)\n",
        "axes[1, 1].plot(y_test_original[:100], label='Actual', linewidth=2, alpha=0.7)\n",
        "axes[1, 1].plot(rnn_predictions[:100], label='RNN Predicted', alpha=0.7)\n",
        "axes[1, 1].plot(transformer_predictions[:100], label='Transformer Predicted', alpha=0.7)\n",
        "axes[1, 1].set_xlabel('Sample')\n",
        "axes[1, 1].set_ylabel('Price')\n",
        "axes[1, 1].set_title('Predictions Comparison (First 100 points)')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22896627",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 5: ANALYSIS (2 MARKS)\n",
        "================================================================================\n",
        "\n",
        "REQUIRED:\n",
        "- Write MAXIMUM 200 words (guideline - no marks deduction if exceeded)\n",
        "- Address key topics with depth\n",
        "\n",
        "GRADING (Quality-based):\n",
        "- Covers 5+ key topics with deep understanding: 2 marks\n",
        "- Covers 3-4 key topics with good understanding: 1 mark\n",
        "- Covers <3 key topics or superficial: 0 marks\n",
        "\n",
        "Key Topics:\n",
        "1. Performance comparison with specific metrics\n",
        "2. RNN vs Transformer architecture advantages\n",
        "3. Impact of attention mechanism vs recurrent connections\n",
        "4. Long-term dependency handling comparison\n",
        "5. Computational cost comparison\n",
        "6. Convergence behavior differences\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d05be677",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate performance differences\n",
        "mae_diff = ((rnn_mae - transformer_mae) / rnn_mae) * 100\n",
        "rmse_diff = ((rnn_rmse - transformer_rmse) / rnn_rmse) * 100\n",
        "rnn_loss_reduction = ((rnn_initial_loss - rnn_final_loss) / rnn_initial_loss) * 100\n",
        "transformer_loss_reduction = ((transformer_initial_loss - transformer_final_loss) / transformer_initial_loss) * 100\n",
        "\n",
        "analysis_text = f\"\"\"\n",
        "Performance Comparison: The Transformer model demonstrated superior performance with MAE of {transformer_mae:.4f} compared to RNN's {rnn_mae:.4f} ({mae_diff:.1f}% improvement) and RMSE of {transformer_rmse:.4f} vs {rnn_rmse:.4f} ({rmse_diff:.1f}% improvement). The R² score for Transformer ({transformer_r2:.4f}) was higher than RNN ({rnn_r2:.4f}), indicating better variance explanation.\n",
        "\n",
        "Architecture Advantages: RNNs process sequences sequentially, making them computationally efficient for streaming data but limiting parallelization. Transformers enable parallel processing of entire sequences through self-attention, significantly accelerating training. However, RNNs have lower memory requirements and are more suitable for online prediction scenarios.\n",
        "\n",
        "Attention Mechanism Impact: Multi-head attention allows the Transformer to simultaneously attend to all positions in the sequence, capturing both short and long-term dependencies effectively. This contrasts with RNNs where information must flow through sequential hidden states, potentially suffering from vanishing gradients.\n",
        "\n",
        "Long-term Dependency Handling: RNNs struggle with long-term dependencies due to gradient vanishing/exploding problems, even with LSTM gates. Transformers excel at modeling long-range dependencies through direct attention connections, enabling better capture of temporal patterns across the entire sequence length.\n",
        "\n",
        "Computational Cost: The Transformer required {transformer_training_time:.2f}s vs RNN's {rnn_training_time:.2f}s for training, with {transformer_total_params} parameters compared to RNN's {rnn_total_params}. While Transformers have more parameters, their parallel architecture can be more efficient on modern hardware.\n",
        "\n",
        "Convergence Behavior: Both models showed good convergence with RNN achieving {rnn_loss_reduction:.1f}% loss reduction and Transformer achieving {transformer_loss_reduction:.1f}% reduction. The Transformer's loss curve was smoother, indicating more stable training dynamics.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05df3e69",
      "metadata": {},
      "outputs": [],
      "source": [
        "# REQUIRED: Print analysis with word count\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(analysis_text)\n",
        "print(\"=\"*70)\n",
        "print(f\"Analysis word count: {len(analysis_text.split())} words\")\n",
        "if len(analysis_text.split()) > 200:\n",
        "    print(\"⚠️  Warning: Analysis exceeds 200 words (guideline)\")\n",
        "else:\n",
        "    print(\"✓ Analysis within word count guideline\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f2ce90",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 6: ASSIGNMENT RESULTS SUMMARY (REQUIRED FOR AUTO-GRADING)\n",
        "================================================================================\n",
        "\n",
        "DO NOT MODIFY THE STRUCTURE BELOW\n",
        "This JSON output is used by the auto-grader\n",
        "Ensure all field names are EXACT\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c800bf2e",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def get_assignment_results():\n",
        "    \"\"\"\n",
        "    Generate complete assignment results in required format\n",
        "    \n",
        "    Returns:\n",
        "        dict: Complete results with all required fields\n",
        "    \"\"\"\n",
        "    \n",
        "    framework_used = \"pytorch\"\n",
        "    rnn_model_type = \"LSTM\"\n",
        "    \n",
        "    results = {\n",
        "        # Dataset Information\n",
        "        'dataset_name': dataset_name,\n",
        "        'dataset_source': dataset_source,\n",
        "        'n_samples': n_samples,\n",
        "        'n_features': n_features,\n",
        "        'sequence_length': sequence_length,\n",
        "        'prediction_horizon': prediction_horizon,\n",
        "        'problem_type': problem_type,\n",
        "        'primary_metric': primary_metric,\n",
        "        'metric_justification': metric_justification,\n",
        "        'train_samples': train_samples,\n",
        "        'test_samples': test_samples,\n",
        "        'train_test_ratio': train_test_ratio,\n",
        "        \n",
        "        # RNN Model Results\n",
        "        'rnn_model': {\n",
        "            'framework': framework_used,\n",
        "            'model_type': rnn_model_type,\n",
        "            'architecture': {\n",
        "                'n_layers': 2,  # Number of stacked layers\n",
        "                'hidden_units': 64,  # Hidden units per layer\n",
        "                'total_parameters': int(rnn_total_params)  # Total parameters\n",
        "            },\n",
        "            'training_config': {\n",
        "                'learning_rate': 0.001,  # Actual learning rate\n",
        "                'n_epochs': 50,  # Actual epochs\n",
        "                'batch_size': 32,  # Actual batch size\n",
        "                'optimizer': 'Adam',  # Actual optimizer\n",
        "                'loss_function': 'MSE'  # Actual loss\n",
        "            },\n",
        "            'initial_loss': float(rnn_initial_loss),\n",
        "            'final_loss': float(rnn_final_loss),\n",
        "            'training_time_seconds': float(rnn_training_time),\n",
        "            'mae': float(rnn_mae),\n",
        "            'rmse': float(rnn_rmse),\n",
        "            'mape': float(rnn_mape),\n",
        "            'r2_score': float(rnn_r2)\n",
        "        },\n",
        "        \n",
        "        # Transformer Model Results\n",
        "        'transformer_model': {\n",
        "            'framework': framework_used,\n",
        "            'architecture': {\n",
        "                'n_layers': 2,  # Number of transformer layers\n",
        "                'n_heads': 4,  # Number of attention heads\n",
        "                'd_model': 64,  # Model dimension\n",
        "                'd_ff': 256,  # Feed-forward dimension\n",
        "                'has_positional_encoding': True,  # MUST be True\n",
        "                'has_attention': True,  # MUST be True\n",
        "                'total_parameters': int(transformer_total_params)  # Total parameters\n",
        "            },\n",
        "            'training_config': {\n",
        "                'learning_rate': 0.001,  # Actual learning rate\n",
        "                'n_epochs': 50,  # Actual epochs\n",
        "                'batch_size': 32,  # Actual batch size\n",
        "                'optimizer': 'Adam',  # Actual optimizer\n",
        "                'loss_function': 'MSE'  # Actual loss\n",
        "            },\n",
        "            'initial_loss': float(transformer_initial_loss),\n",
        "            'final_loss': float(transformer_final_loss),\n",
        "            'training_time_seconds': float(transformer_training_time),\n",
        "            'mae': float(transformer_mae),\n",
        "            'rmse': float(transformer_rmse),\n",
        "            'mape': float(transformer_mape),\n",
        "            'r2_score': float(transformer_r2)\n",
        "        },\n",
        "        \n",
        "        # Analysis\n",
        "        'analysis': analysis_text,\n",
        "        'analysis_word_count': len(analysis_text.split()),\n",
        "        \n",
        "        # Training Success Indicators\n",
        "        'rnn_loss_decreased': bool(rnn_final_loss < rnn_initial_loss),\n",
        "        'transformer_loss_decreased': bool(transformer_final_loss < transformer_initial_loss),\n",
        "    }\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "694a274d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate and print results\n",
        "try:\n",
        "    assignment_results = get_assignment_results()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ASSIGNMENT RESULTS SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(json.dumps(assignment_results, indent=2))\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce921018",
      "metadata": {},
      "outputs": [],
      "source": [
        "except Exception as e:\n",
        "    print(f\"\\n⚠️  ERROR generating results: {str(e)}\")\n",
        "    print(\"Please ensure all variables are properly defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60f84f5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "FINAL CHECKLIST - VERIFY BEFORE SUBMISSION\n",
        "================================================================================\n",
        "\n",
        "□ Student information filled at the top (BITS ID, Name, Email)\n",
        "□ Filename is <BITS_ID>_rnn_assignment.ipynb\n",
        "□ All cells executed (Kernel → Restart & Run All)\n",
        "□ All outputs visible\n",
        "□ LSTM/GRU implemented with stacked layers\n",
        "□ Positional encoding implemented (sinusoidal)\n",
        "□ Multi-head attention implemented (Q, K, V, scaled dot-product)\n",
        "□ Both models use Keras or PyTorch\n",
        "□ Both models trained with loss tracking (initial_loss and final_loss)\n",
        "□ All 4 metrics calculated for both models (MAE, RMSE, MAPE, R²)\n",
        "□ Temporal train/test split used (NO shuffling)\n",
        "□ Primary metric selected and justified\n",
        "□ Analysis written (quality matters, not just word count)\n",
        "□ Visualizations created\n",
        "□ Assignment results JSON printed at the end\n",
        "□ No execution errors in any cell\n",
        "□ File opens without corruption\n",
        "□ Submit ONLY .ipynb file (NO zip, NO data files, NO images)\n",
        "□ Screenshot of environment with account details included\n",
        "□ Only one submission attempt\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcf6d14b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "ENVIRONMENT VERIFICATION - SCREENSHOT REQUIRED\n",
        "================================================================================\n",
        "\n",
        "IMPORTANT: Take a screenshot of your environment showing account details\n",
        "\n",
        "For Google Colab:\n",
        "- Click on your profile icon (top right)\n",
        "- Screenshot should show your email/account clearly\n",
        "- Include the entire Colab interface with notebook name visible\n",
        "\n",
        "For BITS Virtual Lab:\n",
        "- Screenshot showing your login credentials/account details\n",
        "- Include the entire interface with your username/session info visible\n",
        "\n",
        "Paste the screenshot below this cell or in a new markdown cell.\n",
        "This helps verify the work was done by you in your environment.\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e01e42",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display system information\n",
        "import platform\n",
        "import sys\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a2fac2",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"ENVIRONMENT INFORMATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n⚠️  REQUIRED: Add screenshot of your Google Colab/BITS Virtual Lab\")\n",
        "print(\"showing your account details in the cell below this one.\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
